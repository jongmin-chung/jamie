---
title: "AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•€í…Œí¬ ì‘ìš©: ê°œì¸í™”ë¶€í„° ë¦¬ìŠ¤í¬ ê´€ë¦¬ê¹Œì§€"
description: "ì¸ê³µì§€ëŠ¥ ê¸°ìˆ ì´ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ì–´ë–»ê²Œ í˜ì‹ í•˜ê³  ìˆëŠ”ì§€, ì¹´ì¹´ì˜¤í˜ì´ì˜ AI ì ìš© ì‚¬ë¡€ì™€ í•¨ê»˜ ì‚´í´ë´…ë‹ˆë‹¤."
publishedAt: "2024-12-30"
category: "Tech"
tags: ["AI", "ë¨¸ì‹ ëŸ¬ë‹", "í•€í…Œí¬", "ê°œì¸í™”", "ë¦¬ìŠ¤í¬ê´€ë¦¬"]
author: "ê¹€ì—ì´ì•„ì´"
featured: true
---

# AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•€í…Œí¬ ì‘ìš©: ê°œì¸í™”ë¶€í„° ë¦¬ìŠ¤í¬ ê´€ë¦¬ê¹Œì§€

ì¸ê³µì§€ëŠ¥ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ìˆ ì€ ê¸ˆìœµ ì„œë¹„ìŠ¤ì˜ ëª¨ë“  ì˜ì—­ì„ í˜ì‹ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤í˜ì´ì—ì„œ AI ê¸°ìˆ ì„ í™œìš©í•´ ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ê³  ë¦¬ìŠ¤í¬ë¥¼ ê´€ë¦¬í•˜ë©° ìƒˆë¡œìš´ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•œ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ, í•€í…Œí¬ì—ì„œì˜ AI í™œìš©ë²•ì„ ì‹¬ë„ ìˆê²Œ ì†Œê°œí•©ë‹ˆë‹¤.

## ê°œì¸í™” ì¶”ì²œ ì‹œìŠ¤í…œ

### 1. ì‚¬ìš©ì í–‰ë™ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
```python
import numpy as np
import pandas as pd
from sklearn.decomposition import NMF
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Concatenate, Dropout

# í˜‘ì—… í•„í„°ë§ ëª¨ë¸
class PaymentRecommendationEngine:
    def __init__(self):
        self.user_merchant_matrix = None
        self.merchant_features = None
        self.user_embeddings = None
        self.merchant_embeddings = None
        
    def prepare_data(self, transactions_df):
        """ê±°ë˜ ë°ì´í„°ë¥¼ ì¶”ì²œ ì‹œìŠ¤í…œìš©ìœ¼ë¡œ ì „ì²˜ë¦¬"""
        # ì‚¬ìš©ì-ê°€ë§¹ì  í–‰ë ¬ ìƒì„±
        self.user_merchant_matrix = transactions_df.pivot_table(
            index='user_id', 
            columns='merchant_id', 
            values='amount',
            aggfunc='sum',
            fill_value=0
        )
        
        # ê°€ë§¹ì  íŠ¹ì„± ì¶”ì¶œ
        merchant_stats = transactions_df.groupby('merchant_id').agg({
            'amount': ['mean', 'std', 'count'],
            'category': lambda x: x.iloc[0],  # ì¹´í…Œê³ ë¦¬
            'user_id': 'nunique'  # ê³ ìœ  ì‚¬ìš©ì ìˆ˜
        }).reset_index()
        
        # ì¹´í…Œê³ ë¦¬ ë²¡í„°í™”
        tfidf = TfidfVectorizer(max_features=100)
        category_features = tfidf.fit_transform(
            merchant_stats['category'].astype(str)
        ).toarray()
        
        self.merchant_features = np.hstack([
            merchant_stats[['amount']].values,
            category_features
        ])
        
    def build_neural_collaborative_filtering(self, n_users, n_merchants, embedding_dim=50):
        """Neural Collaborative Filtering ëª¨ë¸ êµ¬ì¶•"""
        # ì…ë ¥ ë ˆì´ì–´
        user_input = Input(shape=(), name='user_id')
        merchant_input = Input(shape=(), name='merchant_id')
        
        # ì„ë² ë”© ë ˆì´ì–´
        user_embedding = Embedding(
            n_users, embedding_dim, 
            name='user_embedding'
        )(user_input)
        merchant_embedding = Embedding(
            n_merchants, embedding_dim,
            name='merchant_embedding' 
        )(merchant_input)
        
        # ë²¡í„° í‰íƒ„í™”
        user_vec = tf.keras.layers.Flatten()(user_embedding)
        merchant_vec = tf.keras.layers.Flatten()(merchant_embedding)
        
        # MLP ê²½ë¡œ
        mlp_concat = Concatenate()([user_vec, merchant_vec])
        mlp_dropout = Dropout(0.2)(mlp_concat)
        mlp_dense1 = Dense(128, activation='relu')(mlp_dropout)
        mlp_dropout2 = Dropout(0.2)(mlp_dense1)
        mlp_dense2 = Dense(64, activation='relu')(mlp_dropout2)
        
        # GMF (Generalized Matrix Factorization) ê²½ë¡œ
        gmf_layer = tf.keras.layers.Multiply()([user_vec, merchant_vec])
        
        # ìµœì¢… ê²°í•©
        final_concat = Concatenate()([mlp_dense2, gmf_layer])
        output = Dense(1, activation='sigmoid', name='rating')(final_concat)
        
        model = Model(inputs=[user_input, merchant_input], outputs=output)
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['mae', 'mse']
        )
        
        return model
    
    def train_model(self, model, train_data, validation_data, epochs=50):
        """ëª¨ë¸ í›ˆë ¨"""
        # ì½œë°± ì„¤ì •
        callbacks = [
            tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=5,
                restore_best_weights=True
            ),
            tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3
            )
        ]
        
        history = model.fit(
            train_data,
            validation_data=validation_data,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def get_user_recommendations(self, model, user_id, n_recommendations=10):
        """ì‚¬ìš©ìë³„ ì¶”ì²œ ê°€ë§¹ì  ìƒì„±"""
        # ëª¨ë“  ê°€ë§¹ì ì— ëŒ€í•œ ì˜ˆì¸¡
        all_merchants = range(len(self.merchant_features))
        user_array = np.array([user_id] * len(all_merchants))
        merchant_array = np.array(all_merchants)
        
        predictions = model.predict([user_array, merchant_array])
        
        # ê¸°ì¡´ ì´ìš© ê°€ë§¹ì  ì œì™¸
        used_merchants = set(
            self.user_merchant_matrix.loc[user_id]
            [self.user_merchant_matrix.loc[user_id] > 0].index
        )
        
        # ì¶”ì²œ ì ìˆ˜ ê³„ì‚° ë° ì •ë ¬
        recommendations = []
        for i, (merchant_id, score) in enumerate(zip(all_merchants, predictions.flatten())):
            if merchant_id not in used_merchants:
                recommendations.append((merchant_id, score))
        
        recommendations.sort(key=lambda x: x[1], reverse=True)
        return recommendations[:n_recommendations]
```

### 2. ì‹¤ì‹œê°„ ê°œì¸í™” ì„œë¹„ìŠ¤
```python
import redis
import json
from datetime import datetime, timedelta
import asyncio

class RealTimePersonalizationEngine:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.feature_cache_ttl = 3600  # 1ì‹œê°„
        self.recommendation_cache_ttl = 1800  # 30ë¶„
        
    async def update_user_features(self, user_id, transaction_data):
        """ì‹¤ì‹œê°„ ì‚¬ìš©ì íŠ¹ì„± ì—…ë°ì´íŠ¸"""
        feature_key = f"user_features:{user_id}"
        
        # ê¸°ì¡´ íŠ¹ì„± ì¡°íšŒ
        existing_features = self.redis.hgetall(feature_key)
        
        # ìƒˆë¡œìš´ íŠ¹ì„± ê³„ì‚°
        new_features = self.calculate_features(transaction_data, existing_features)
        
        # Redisì— ì—…ë°ì´íŠ¸
        pipeline = self.redis.pipeline()
        for feature, value in new_features.items():
            pipeline.hset(feature_key, feature, json.dumps(value))
        pipeline.expire(feature_key, self.feature_cache_ttl)
        await pipeline.execute()
        
        # ì¶”ì²œ ìºì‹œ ë¬´íš¨í™”
        await self.invalidate_recommendation_cache(user_id)
    
    def calculate_features(self, transaction_data, existing_features):
        """ì‚¬ìš©ì íŠ¹ì„± ê³„ì‚°"""
        features = {}
        
        # ê±°ë˜ ë¹ˆë„ íŠ¹ì„±
        features['avg_transaction_amount'] = transaction_data.get('amount', 0)
        features['transaction_count'] = existing_features.get('transaction_count', 0) + 1
        features['last_transaction_time'] = datetime.now().isoformat()
        
        # ì¹´í…Œê³ ë¦¬ ì„ í˜¸ë„
        category = transaction_data.get('category')
        if category:
            category_key = f'category_preference_{category}'
            current_pref = json.loads(existing_features.get(category_key, '0'))
            features[category_key] = current_pref + 1
        
        # ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´
        hour = datetime.now().hour
        time_slot = self.get_time_slot(hour)
        time_key = f'time_preference_{time_slot}'
        current_time_pref = json.loads(existing_features.get(time_key, '0'))
        features[time_key] = current_time_pref + 1
        
        return features
    
    def get_time_slot(self, hour):
        """ì‹œê°„ì„ ìŠ¬ë¡¯ìœ¼ë¡œ ë³€í™˜"""
        if 6 <= hour < 12:
            return 'morning'
        elif 12 <= hour < 18:
            return 'afternoon'
        elif 18 <= hour < 22:
            return 'evening'
        else:
            return 'night'
    
    async def get_personalized_offers(self, user_id, context=None):
        """ê°œì¸í™”ëœ ì˜¤í¼ ì œê³µ"""
        cache_key = f"offers:{user_id}"
        
        # ìºì‹œëœ ì¶”ì²œ í™•ì¸
        cached_offers = await self.redis.get(cache_key)
        if cached_offers:
            return json.loads(cached_offers)
        
        # ì‚¬ìš©ì íŠ¹ì„± ì¡°íšŒ
        user_features = await self.redis.hgetall(f"user_features:{user_id}")
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
        if context:
            current_location = context.get('location')
            current_time = context.get('time', datetime.now())
            time_slot = self.get_time_slot(current_time.hour)
        
        # ê°œì¸í™”ëœ ì˜¤í¼ ìƒì„±
        offers = await self.generate_contextual_offers(
            user_features, current_location, time_slot
        )
        
        # ê²°ê³¼ ìºì‹±
        await self.redis.setex(
            cache_key, 
            self.recommendation_cache_ttl,
            json.dumps(offers)
        )
        
        return offers
    
    async def generate_contextual_offers(self, user_features, location, time_slot):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜¤í¼ ìƒì„±"""
        offers = []
        
        # ìœ„ì¹˜ ê¸°ë°˜ ì˜¤í¼
        if location:
            nearby_merchants = await self.get_nearby_merchants(location)
            for merchant in nearby_merchants:
                offer_score = self.calculate_offer_score(
                    merchant, user_features, time_slot
                )
                if offer_score > 0.7:  # ì„ê³„ê°’
                    offers.append({
                        'merchant_id': merchant['id'],
                        'merchant_name': merchant['name'],
                        'offer_type': 'location_based',
                        'score': offer_score,
                        'distance': merchant['distance']
                    })
        
        # ì‹œê°„ëŒ€ë³„ ì˜¤í¼
        time_preference_key = f'time_preference_{time_slot}'
        if time_preference_key in user_features:
            time_based_offers = await self.get_time_based_offers(time_slot)
            offers.extend(time_based_offers)
        
        return sorted(offers, key=lambda x: x['score'], reverse=True)[:10]
```

## ë¦¬ìŠ¤í¬ ê´€ë¦¬ AI

### 1. ì´ìƒê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ
```python
import pandas as pd
import numpy as np
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import xgboost as xgb
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import joblib

class FraudDetectionSystem:
    def __init__(self):
        self.isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        self.xgb_model = None
        self.lstm_model = None
        self.scaler = StandardScaler()
        self.feature_columns = None
        
    def preprocess_features(self, transactions_df):
        """ê±°ë˜ ë°ì´í„° íŠ¹ì„± ì¶”ì¶œ"""
        # ê¸°ë³¸ íŠ¹ì„±
        features = transactions_df.copy()
        
        # ì‹œê°„ íŠ¹ì„±
        features['hour'] = pd.to_datetime(features['timestamp']).dt.hour
        features['day_of_week'] = pd.to_datetime(features['timestamp']).dt.dayofweek
        features['is_weekend'] = features['day_of_week'].isin([5, 6]).astype(int)
        
        # ì‚¬ìš©ìë³„ í†µê³„ íŠ¹ì„±
        user_stats = transactions_df.groupby('user_id').agg({
            'amount': ['mean', 'std', 'count', 'max'],
            'merchant_id': 'nunique'
        }).reset_index()
        user_stats.columns = ['user_id', 'user_avg_amount', 'user_std_amount', 
                             'user_tx_count', 'user_max_amount', 'user_unique_merchants']
        
        features = features.merge(user_stats, on='user_id', how='left')
        
        # Z-score íŠ¹ì„± (í‰ì†Œì™€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€)
        features['amount_zscore'] = (
            features['amount'] - features['user_avg_amount']
        ) / (features['user_std_amount'] + 1e-8)
        
        # ê°€ë§¹ì ë³„ í†µê³„
        merchant_stats = transactions_df.groupby('merchant_id').agg({
            'amount': ['mean', 'std'],
            'user_id': 'nunique'
        }).reset_index()
        merchant_stats.columns = ['merchant_id', 'merchant_avg_amount', 
                                'merchant_std_amount', 'merchant_unique_users']
        
        features = features.merge(merchant_stats, on='merchant_id', how='left')
        
        # ìˆœì°¨ íŠ¹ì„± (ìµœê·¼ ê±°ë˜ íŒ¨í„´)
        features = features.sort_values(['user_id', 'timestamp'])
        features['time_since_last_tx'] = (
            features.groupby('user_id')['timestamp']
            .diff().dt.total_seconds().fillna(0)
        )
        
        # ì†ë„ íŠ¹ì„± (ì—°ì† ê±°ë˜)
        features['tx_velocity_1h'] = (
            features.groupby('user_id')['timestamp']
            .rolling('1H', on='timestamp').count()
        )
        
        return features
    
    def train_isolation_forest(self, features_df):
        """Isolation Forestë¥¼ ì‚¬ìš©í•œ ì´ìƒì¹˜ íƒì§€"""
        feature_cols = [
            'amount', 'amount_zscore', 'hour', 'day_of_week',
            'user_avg_amount', 'user_std_amount', 'time_since_last_tx',
            'tx_velocity_1h'
        ]
        
        X = features_df[feature_cols].fillna(0)
        X_scaled = self.scaler.fit_transform(X)
        
        self.isolation_forest.fit(X_scaled)
        
        # ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚°
        anomaly_scores = self.isolation_forest.decision_function(X_scaled)
        features_df['anomaly_score'] = anomaly_scores
        
        return features_df
    
    def train_xgboost_classifier(self, features_df, labels):
        """XGBoostë¥¼ ì‚¬ìš©í•œ ì‚¬ê¸° ë¶„ë¥˜"""
        feature_cols = [
            'amount', 'amount_zscore', 'hour', 'day_of_week', 'is_weekend',
            'user_avg_amount', 'user_std_amount', 'user_tx_count',
            'merchant_avg_amount', 'merchant_std_amount', 'time_since_last_tx',
            'tx_velocity_1h', 'anomaly_score'
        ]
        
        X = features_df[feature_cols].fillna(0)
        y = labels
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # XGBoost ëª¨ë¸ í›ˆë ¨
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])
        )
        
        self.xgb_model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            early_stopping_rounds=10,
            verbose=False
        )
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ì €ì¥
        self.feature_importance = dict(zip(
            feature_cols,
            self.xgb_model.feature_importances_
        ))
        
        return self.xgb_model
    
    def build_lstm_model(self, sequence_length, n_features):
        """LSTMì„ ì‚¬ìš©í•œ ìˆœì°¨ íŒ¨í„´ ë¶„ì„"""
        model = Sequential([
            LSTM(64, return_sequences=True, input_shape=(sequence_length, n_features)),
            Dropout(0.2),
            LSTM(32, return_sequences=False),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer='adam',
            loss='binary_crossentropy',
            metrics=['precision', 'recall', 'f1_score']
        )
        
        return model
    
    def create_sequences(self, user_transactions, sequence_length=10):
        """ì‚¬ìš©ìë³„ ê±°ë˜ ì‹œí€€ìŠ¤ ìƒì„±"""
        sequences = []
        labels = []
        
        for user_id, group in user_transactions.groupby('user_id'):
            if len(group) < sequence_length:
                continue
                
            group = group.sort_values('timestamp')
            feature_cols = ['amount', 'hour', 'day_of_week', 'amount_zscore']
            
            for i in range(len(group) - sequence_length):
                sequence = group[feature_cols].iloc[i:i+sequence_length].values
                label = group['is_fraud'].iloc[i+sequence_length]
                
                sequences.append(sequence)
                labels.append(label)
        
        return np.array(sequences), np.array(labels)
    
    def predict_fraud_probability(self, transaction_features):
        """ê±°ë˜ì˜ ì‚¬ê¸° í™•ë¥  ì˜ˆì¸¡"""
        # Isolation Forest ì´ìƒì¹˜ ì ìˆ˜
        X_scaled = self.scaler.transform([transaction_features])
        anomaly_score = self.isolation_forest.decision_function(X_scaled)[0]
        
        # XGBoost ë¶„ë¥˜ í™•ë¥ 
        features_with_anomaly = transaction_features + [anomaly_score]
        xgb_prob = self.xgb_model.predict_proba([features_with_anomaly])[0][1]
        
        # ì•™ìƒë¸” ì ìˆ˜ (ê°€ì¤‘ í‰ê· )
        ensemble_score = 0.7 * xgb_prob + 0.3 * (1 - (anomaly_score + 1) / 2)
        
        return {
            'fraud_probability': ensemble_score,
            'anomaly_score': anomaly_score,
            'xgb_probability': xgb_prob,
            'risk_level': self.categorize_risk(ensemble_score)
        }
    
    def categorize_risk(self, probability):
        """ìœ„í—˜ë„ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if probability < 0.3:
            return 'LOW'
        elif probability < 0.7:
            return 'MEDIUM'
        else:
            return 'HIGH'
```

### 2. ì‹ ìš©í‰ê°€ AI ëª¨ë¸
```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import roc_auc_score, classification_report
import lightgbm as lgb
import shap

class CreditScoringModel:
    def __init__(self):
        self.models = {
            'logistic': LogisticRegression(random_state=42),
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingClassifier(random_state=42),
            'lightgbm': lgb.LGBMClassifier(random_state=42)
        }
        self.best_model = None
        self.feature_names = None
        self.explainer = None
        
    def prepare_credit_features(self, user_data, transaction_data, external_data=None):
        """ì‹ ìš©í‰ê°€ìš© íŠ¹ì„± ìƒì„±"""
        features = {}
        
        # ê¸°ë³¸ ì¸ì ì‚¬í•­
        features.update({
            'age': user_data.get('age', 0),
            'income': user_data.get('annual_income', 0),
            'employment_duration': user_data.get('employment_duration_months', 0),
            'education_level': user_data.get('education_level', 0)  # encoded
        })
        
        # ê±°ë˜ í–‰ë™ íŠ¹ì„±
        if not transaction_data.empty:
            tx_stats = self.calculate_transaction_stats(transaction_data)
            features.update(tx_stats)
        
        # ì™¸ë¶€ ë°ì´í„° (ì„ íƒì )
        if external_data:
            features.update({
                'credit_bureau_score': external_data.get('credit_score', 0),
                'existing_loans': external_data.get('loan_count', 0),
                'debt_to_income_ratio': external_data.get('debt_ratio', 0)
            })
        
        return features
    
    def calculate_transaction_stats(self, transactions):
        """ê±°ë˜ ë°ì´í„° ê¸°ë°˜ í†µê³„ íŠ¹ì„±"""
        stats = {}
        
        # ê¸°ë³¸ ê±°ë˜ í†µê³„
        stats['total_transactions'] = len(transactions)
        stats['avg_transaction_amount'] = transactions['amount'].mean()
        stats['std_transaction_amount'] = transactions['amount'].std()
        stats['max_transaction_amount'] = transactions['amount'].max()
        stats['total_volume'] = transactions['amount'].sum()
        
        # ê±°ë˜ ë¹ˆë„ ë¶„ì„
        transactions['date'] = pd.to_datetime(transactions['timestamp']).dt.date
        daily_tx = transactions.groupby('date').size()
        stats['avg_daily_transactions'] = daily_tx.mean()
        stats['max_daily_transactions'] = daily_tx.max()
        
        # ê±°ë˜ ë‹¤ì–‘ì„±
        stats['unique_merchants'] = transactions['merchant_id'].nunique()
        stats['merchant_concentration'] = (
            transactions['merchant_id'].value_counts().iloc[0] / len(transactions)
            if len(transactions) > 0 else 0
        )
        
        # ì‹œê°„ íŒ¨í„´
        transactions['hour'] = pd.to_datetime(transactions['timestamp']).dt.hour
        stats['night_tx_ratio'] = (
            len(transactions[(transactions['hour'] < 6) | (transactions['hour'] > 22)]) / 
            len(transactions) if len(transactions) > 0 else 0
        )
        
        # ì¹´í…Œê³ ë¦¬ ë¶„ì‚°
        category_dist = transactions['category'].value_counts(normalize=True)
        stats['category_entropy'] = -sum(p * np.log2(p) for p in category_dist)
        
        # ì •ê¸° ê²°ì œ íŒ¨í„´
        monthly_amounts = transactions.groupby(
            pd.to_datetime(transactions['timestamp']).dt.to_period('M')
        )['amount'].sum()
        stats['payment_stability'] = 1 - (monthly_amounts.std() / monthly_amounts.mean())
        
        return stats
    
    def train_ensemble_model(self, X, y):
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨ ë° ìµœì  ëª¨ë¸ ì„ íƒ"""
        model_scores = {}
        
        for name, model in self.models.items():
            # êµì°¨ê²€ì¦ ì ìˆ˜
            cv_scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')
            model_scores[name] = cv_scores.mean()
            
            # ëª¨ë¸ í›ˆë ¨
            model.fit(X, y)
            
            print(f"{name} CV AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì„ íƒ
        best_model_name = max(model_scores, key=model_scores.get)
        self.best_model = self.models[best_model_name]
        self.feature_names = list(X.columns)
        
        print(f"Best model: {best_model_name} with AUC: {model_scores[best_model_name]:.4f}")
        
        # SHAP ì„¤ëª… ëª¨ë¸ ìƒì„±
        self.explainer = shap.TreeExplainer(self.best_model)
        
        return self.best_model
    
    def predict_credit_score(self, features):
        """ì‹ ìš©ì ìˆ˜ ì˜ˆì¸¡"""
        feature_array = np.array([list(features.values())])
        
        # í™•ë¥  ì˜ˆì¸¡
        probability = self.best_model.predict_proba(feature_array)[0][1]
        
        # ì‹ ìš©ì ìˆ˜ ë³€í™˜ (300-850 ë²”ìœ„)
        credit_score = int(300 + probability * 550)
        
        # ì‹ ìš©ë“±ê¸‰ ê²°ì •
        credit_grade = self.get_credit_grade(credit_score)
        
        # SHAP ì„¤ëª…
        shap_values = self.explainer.shap_values(feature_array)[1]
        feature_importance = dict(zip(self.feature_names, shap_values[0]))
        
        return {
            'credit_score': credit_score,
            'credit_grade': credit_grade,
            'approval_probability': probability,
            'feature_importance': feature_importance,
            'risk_factors': self.identify_risk_factors(feature_importance)
        }
    
    def get_credit_grade(self, score):
        """ì‹ ìš©ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜"""
        if score >= 750:
            return 'AAA'
        elif score >= 700:
            return 'AA'
        elif score >= 650:
            return 'A'
        elif score >= 600:
            return 'BBB'
        elif score >= 550:
            return 'BB'
        elif score >= 500:
            return 'B'
        else:
            return 'C'
    
    def identify_risk_factors(self, feature_importance, top_n=5):
        """ì£¼ìš” ìœ„í—˜ ìš”ì†Œ ì‹ë³„"""
        # ìŒìˆ˜ ì˜í–¥ì„ ì£¼ëŠ” íŠ¹ì„±ë“¤ (ìœ„í—˜ ìš”ì†Œ)
        risk_factors = {k: v for k, v in feature_importance.items() if v < 0}
        
        # ì ˆëŒ“ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_risks = sorted(risk_factors.items(), key=lambda x: abs(x[1]), reverse=True)
        
        return dict(sorted_risks[:top_n])
```

## ëŒ€í™”í˜• AIì™€ ì±—ë´‡

### 1. ê¸ˆìœµ ìƒë‹´ ì±—ë´‡
```python
import openai
from transformers import pipeline
import re
import json
from datetime import datetime

class FinancialChatbot:
    def __init__(self, api_key):
        openai.api_key = api_key
        self.intent_classifier = pipeline(
            "text-classification",
            model="bert-base-multilingual-cased"
        )
        self.conversation_history = {}
        self.financial_context = {}
        
    def classify_intent(self, message):
        """ì‚¬ìš©ì ì˜ë„ ë¶„ë¥˜"""
        intents = {
            'balance_inquiry': ['ì”ì•¡', 'ì–¼ë§ˆ', 'ëˆ', 'ê³„ì¢Œ'],
            'transaction_history': ['ë‚´ì—­', 'ê±°ë˜', 'ì‚¬ìš©', 'ê²°ì œ'],
            'payment_method': ['ì¹´ë“œ', 'ê³„ì¢Œ', 'ë“±ë¡', 'ê²°ì œìˆ˜ë‹¨'],
            'loan_inquiry': ['ëŒ€ì¶œ', 'í•œë„', 'ì‹ ìš©', 'ë¹Œë ¤'],
            'investment': ['íˆ¬ì', 'ì ê¸ˆ', 'ì£¼ì‹', 'í€ë“œ'],
            'complaint': ['ë¶ˆë§Œ', 'ë¬¸ì œ', 'ì˜¤ë¥˜', 'ì˜ëª»'],
            'general_inquiry': ['ì •ë³´', 'ë¬¸ì˜', 'ê¶ê¸ˆ', 'ì•Œë ¤']
        }
        
        message_lower = message.lower()
        for intent, keywords in intents.items():
            if any(keyword in message_lower for keyword in keywords):
                return intent
        
        return 'general_inquiry'
    
    def extract_entities(self, message):
        """ì—”í‹°í‹° ì¶”ì¶œ (ê¸ˆì•¡, ë‚ ì§œ ë“±)"""
        entities = {}
        
        # ê¸ˆì•¡ ì¶”ì¶œ
        amount_pattern = r'(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)(?:ì›|ë§Œì›|ì–µì›)?'
        amounts = re.findall(amount_pattern, message)
        if amounts:
            entities['amount'] = amounts[0].replace(',', '')
        
        # ë‚ ì§œ ì¶”ì¶œ
        date_patterns = [
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            r'(ì–´ì œ|ì˜¤ëŠ˜|ë‚´ì¼|ì´ë²ˆë‹¬|ì§€ë‚œë‹¬)'
        ]
        
        for pattern in date_patterns:
            date_match = re.search(pattern, message)
            if date_match:
                entities['date'] = date_match.group()
                break
        
        return entities
    
    async def process_message(self, user_id, message):
        """ë©”ì‹œì§€ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±"""
        # ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
        if user_id not in self.conversation_history:
            self.conversation_history[user_id] = []
        
        # ì˜ë„ ë° ì—”í‹°í‹° ë¶„ì„
        intent = self.classify_intent(message)
        entities = self.extract_entities(message)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‘ë‹µ ìƒì„±
        context = {
            'intent': intent,
            'entities': entities,
            'user_id': user_id,
            'conversation_history': self.conversation_history[user_id][-5:],  # ìµœê·¼ 5ê°œ
            'timestamp': datetime.now().isoformat()
        }
        
        response = await self.generate_response(context)
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        self.conversation_history[user_id].append({
            'user_message': message,
            'bot_response': response,
            'intent': intent,
            'entities': entities,
            'timestamp': context['timestamp']
        })
        
        return response
    
    async def generate_response(self, context):
        """GPTë¥¼ í™œìš©í•œ ì‘ë‹µ ìƒì„±"""
        intent = context['intent']
        
        # ì˜ë„ë³„ ì „ë¬¸ ì‘ë‹µ ìƒì„±
        if intent == 'balance_inquiry':
            return await self.handle_balance_inquiry(context)
        elif intent == 'transaction_history':
            return await self.handle_transaction_inquiry(context)
        elif intent == 'loan_inquiry':
            return await self.handle_loan_inquiry(context)
        else:
            return await self.handle_general_inquiry(context)
    
    async def handle_balance_inquiry(self, context):
        """ì”ì•¡ ì¡°íšŒ ì²˜ë¦¬"""
        user_id = context['user_id']
        
        # ì‹¤ì œ ì”ì•¡ ì¡°íšŒ (ê°€ìƒ ë°ì´í„°)
        balance = await self.get_user_balance(user_id)
        
        response_template = f"""
        ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ì”ì•¡ì„ ì•ˆë‚´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.

        ğŸ’° ì¹´ì¹´ì˜¤í˜ì´ ë¨¸ë‹ˆ: {balance['kakao_money']:,}ì›
        ğŸ’³ ì—°ê²°ê³„ì¢Œ ì”ì•¡: {balance['linked_account']:,}ì›

        ì¶”ê°€ë¡œ ê¶ê¸ˆí•œ ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë§ì”€í•´ ì£¼ì„¸ìš”.
        """
        
        return response_template.strip()
    
    async def handle_loan_inquiry(self, context):
        """ëŒ€ì¶œ ë¬¸ì˜ ì²˜ë¦¬"""
        user_id = context['user_id']
        entities = context['entities']
        
        # ì‚¬ìš©ì ì‹ ìš©ì •ë³´ ì¡°íšŒ
        credit_info = await self.get_user_credit_info(user_id)
        
        # GPTë¥¼ í™œìš©í•œ ê°œì¸í™” ì‘ë‹µ
        prompt = f"""
        ì‚¬ìš©ìì˜ ëŒ€ì¶œ ë¬¸ì˜ì— ëŒ€í•´ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
        
        ì‚¬ìš©ì ì •ë³´:
        - ì‹ ìš©ë“±ê¸‰: {credit_info.get('grade', 'BB')}
        - ì—°ë´‰: {credit_info.get('income', 0):,}ì›
        - ê¸°ì¡´ ëŒ€ì¶œ: {credit_info.get('existing_loans', 0):,}ì›
        
        ìš”ì²­ ê¸ˆì•¡: {entities.get('amount', 'ì—†ìŒ')}
        
        ì¹´ì¹´ì˜¤ë±…í¬ì˜ ëŒ€ì¶œ ìƒí’ˆì„ ì¶”ì²œí•˜ê³ , ëŒ€ëµì ì¸ í•œë„ì™€ ê¸ˆë¦¬ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
        """
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹´ì¹´ì˜¤í˜ì´ì˜ ì „ë¬¸ ê¸ˆìœµ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ]
        )
        
        return response.choices[0].message.content
    
    async def get_user_balance(self, user_id):
        """ì‚¬ìš©ì ì”ì•¡ ì¡°íšŒ (ëª¨ì˜)"""
        return {
            'kakao_money': 150000,
            'linked_account': 2500000
        }
    
    async def get_user_credit_info(self, user_id):
        """ì‚¬ìš©ì ì‹ ìš©ì •ë³´ ì¡°íšŒ (ëª¨ì˜)"""
        return {
            'grade': 'A',
            'income': 50000000,
            'existing_loans': 30000000
        }
```

AIì™€ ë¨¸ì‹ ëŸ¬ë‹ì€ í•€í…Œí¬ ì„œë¹„ìŠ¤ì˜ ëª¨ë“  ì¸¡ë©´ì„ í˜ì‹ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê°œì¸í™”ëœ ì„œë¹„ìŠ¤ ì œê³µë¶€í„° ë¦¬ìŠ¤í¬ ê´€ë¦¬, ê³ ê° ìƒë‹´ê¹Œì§€ ë‹¤ì–‘í•œ ì˜ì—­ì—ì„œ AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜ê³¼ ì•ˆì „í•œ ê¸ˆìœµ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.